{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from flygym import Fly, Simulation, Camera, get_data_path\n",
    "from flygym.arena import FlatTerrain\n",
    "from flygym.preprogrammed import all_leg_dofs\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "#sys.path.append('C:/Users/laeti/Documents/laetitia/EPFL/Master/CoursMA2/Control_Behaviour/projet')\n",
    "from hybrid_turning_fly import HybridTurningFly\n",
    "from simulation_CPG import Simulation_CPG\n",
    "from flygym.examples.cpg_controller import CPGNetwork\n",
    "from IPython import display\n",
    "from flygym.vision import save_video_with_vision_insets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/flygymv1/flygym/core.py:331: UserWarning: Deprecation warning: The `NeuroMechFly` class has been restructured into `Simulation`, `Fly`, and `Camera`.`NeuroMechFly` will be removed in future versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from flygym import Parameters, NeuroMechFly\n",
    "nmf = NeuroMechFly(Parameters(enable_adhesion=True, draw_adhesion=True))\n",
    "from flygym.examples.common import PreprogrammedSteps\n",
    "\n",
    "#Filtering initialization :\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "preprogrammed_steps = PreprogrammedSteps()\n",
    "swing_periods = preprogrammed_steps.swing_period\n",
    "legs = preprogrammed_steps.legs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTaxisFly(HybridTurningFly):\n",
    "    def __init__(self, obj_threshold=0.15, decision_interval=0.05, **kwargs):\n",
    "        super().__init__(**kwargs, enable_vision=True)\n",
    "        self.obj_threshold = obj_threshold\n",
    "        self.decision_interval = decision_interval\n",
    "        self.num_substeps = int(self.decision_interval / self.timestep)\n",
    "        self.visual_inputs_hist = []\n",
    "        self.standing=[]\n",
    "\n",
    "        self.coms = np.empty((self.retina.num_ommatidia_per_eye, 2))\n",
    "\n",
    "        for i in range(self.retina.num_ommatidia_per_eye):\n",
    "            mask = self.retina.ommatidia_id_map == i + 1\n",
    "            self.coms[i, :] = np.argwhere(mask).mean(axis=0)\n",
    "\n",
    "    def process_visual_observation(self, vision_input):\n",
    "        features = np.zeros((2, 3))\n",
    "\n",
    "        for i, ommatidia_readings in enumerate(vision_input):\n",
    "            is_obj = ommatidia_readings.max(axis=1) < self.obj_threshold\n",
    "            is_obj_coords = self.coms[is_obj]\n",
    "\n",
    "            if is_obj_coords.shape[0] > 0:\n",
    "                features[i, :2] = is_obj_coords.mean(axis=0)\n",
    "\n",
    "            features[i, 2] = is_obj_coords.shape[0]\n",
    "\n",
    "        features[:, 0] /= self.retina.nrows  # normalize y_center\n",
    "        features[:, 1] /= self.retina.ncols  # normalize x_center\n",
    "        features[:, 2] /= self.retina.num_ommatidia_per_eye  # normalize area\n",
    "        return features.ravel().astype(\"float32\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_ipsilateral_speed(deviation, is_found):\n",
    "        if not is_found:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return np.clip(1 - deviation * 3, 0.4, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingFly(Fly):\n",
    "    def __init__(self, init_pose=\"stretch\", actuated_joints=None, control=\"position\",\n",
    "                 initial_position=None, initial_orientation=None, **kwargs):\n",
    "        super().__init__(**kwargs, init_pose=init_pose, actuated_joints=actuated_joints, control=control,\n",
    "                         spawn_pos=initial_position, spawn_orientation=initial_orientation, enable_vision=True)\n",
    "        self.visual_inputs_hist = []\n",
    "\n",
    "    def simulate_step(self, sim: Simulation, roll_angle: float, yaw_angle: float, side: str='L'):\n",
    "        action = {\"joints\": self.simulate_movement(sim, roll_angle, yaw_angle, side)}\n",
    "        action[\"adhesion\"] = np.array([1,0,1,1,0,1]) #add adhesion \n",
    "        return action\n",
    "\n",
    "    def simulate_movement(self, sim: Simulation, roll_angle: float, yaw_angle: float, side: str, increment: float = 0.00015,):\n",
    "        \n",
    "        joint_pos = self.standing.copy()\n",
    "        joint_angles = preprogrammed_steps.get_joint_angles(\"LM\" if side == \"L\" else \"RM\", 0)\n",
    "        \n",
    "        if side == \"L\":\n",
    "            joint_pos[7:14] = joint_angles\n",
    "            joint_pos[9], joint_pos[8] = roll_angle, yaw_angle  # Setting specific yaw and pitch\n",
    "        else:\n",
    "            joint_pos[28:35] = joint_angles\n",
    "            joint_pos[29], joint_pos[28] = roll_angle, yaw_angle\n",
    "        return joint_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprogrammed_steps = PreprogrammedSteps()\n",
    "\n",
    "# swing_periods = preprogrammed_steps.swing_period\n",
    "\n",
    "# legs = preprogrammed_steps.legs\n",
    "# target_num_steps=10\n",
    "\n",
    "\n",
    "\n",
    "# class MovingFly(Fly):\n",
    "#     def __init__(self, init_pose=\"stretch\", actuated_joints=None, control=\"position\",\n",
    "#                  initial_position=None, initial_orientation=None, **kwargs):\n",
    "#         super().__init__(**kwargs, init_pose=init_pose, actuated_joints=actuated_joints, control=control,\n",
    "#                          spawn_pos=initial_position, spawn_orientation=initial_orientation, enable_vision=True)\n",
    "#         self.visual_inputs_hist = []\n",
    "\n",
    "#     def simulate_step(self, sim: Simulation, yaw_angle: float, side: str, i):\n",
    "#         # Compute indices and stretches only for the current step 'i'\n",
    "#         middle_stance_ids = np.linspace(swing_periods[side + \"M\"][1], 2 * np.pi, target_num_steps)[i]\n",
    "        \n",
    "#         R_midleg_start = preprogrammed_steps.get_joint_angles(\"RM\", swing_periods[\"RM\"][1])\n",
    "#         R_midleg_stretch = np.linspace(np.zeros(len(R_midleg_start)), -R_midleg_start, target_num_steps)[i]\n",
    "\n",
    "#         L_midleg_start = preprogrammed_steps.get_joint_angles(\"LM\", swing_periods[\"LM\"][1])\n",
    "#         L_midleg_stretch = np.linspace(np.zeros(len(L_midleg_start)), -L_midleg_start, target_num_steps)[i]\n",
    "\n",
    "#         joint_angles = self.simulate_movement(sim, yaw_angle, side, middle_stance_ids, R_midleg_stretch, L_midleg_stretch)\n",
    "#         action = {\"joints\": np.array(joint_angles), \"adhesion\": np.array([1, 0, 1, 1, 0, 1])}\n",
    "        \n",
    "#         return action\n",
    "\n",
    "#     def simulate_movement(self, sim: Simulation, yaw_angle: float, side: str, middle_stance_id, R_midleg_stretch, L_midleg_stretch):\n",
    "#         joint_pos = self.obs[\"joints\"][0].copy()\n",
    "#         if side == \"L\":\n",
    "#             midleg_joint_angles = preprogrammed_steps.get_joint_angles(\"LM\", middle_stance_id)\n",
    "#             midleg_joint_angles += L_midleg_stretch\n",
    "#             joint_pos[8:14] = midleg_joint_angles  # Adjust specific joints\n",
    "#         else:\n",
    "#             midleg_joint_angles = preprogrammed_steps.get_joint_angles(\"RM\", middle_stance_id)\n",
    "#             midleg_joint_angles += R_midleg_stretch\n",
    "#             joint_pos[29:35] = midleg_joint_angles  # Adjust specific joints\n",
    "\n",
    "#         return joint_pos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario(start_postion=0):\n",
    "    pos = (0,4.5,0)\n",
    "    orien_fwd = (0,0,np.pi/2)\n",
    "    orien_bcd =(0,0,-np.pi/2)\n",
    "\n",
    "    match start_postion:\n",
    "        case 0 : return pos, orien_fwd\n",
    "        case 1:\n",
    "            pos = (16,4.5,0)\n",
    "            return pos, orien_bcd\n",
    "        case 2:\n",
    "            pos = (16,-4.5,0)\n",
    "            return pos, orien_bcd\n",
    "        case 3 :\n",
    "            return (0,-4.5,0), orien_fwd\n",
    "        case TypeError:\n",
    "            print(\"wrong start_position, default values taken\")\n",
    "            return pos, orien_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 1e-4\n",
    "scenario_fly1=0\n",
    "\n",
    "fly = Fly(\n",
    "    name=\"1\",\n",
    "    init_pose=\"stretch\",\n",
    "    actuated_joints=all_leg_dofs,\n",
    "    control=\"position\",\n",
    "    enable_adhesion=True,\n",
    "    draw_adhesion=False,\n",
    "    spawn_pos= scenario(scenario_fly1)[0],\n",
    "    spawn_orientation = scenario(scenario_fly1)[1]\n",
    ")\n",
    "\n",
    "#need to process visual informations\n",
    "# fly0 = VisualTaxisFly(\n",
    "#     name=\"0\", #static\n",
    "#     timestep=timestep,\n",
    "#     enable_adhesion=True,\n",
    "#     head_stabilization_model=\"thorax\",\n",
    "#     neck_kp=1000,\n",
    "#     spawn_pos=(8,0,0), #position\n",
    "#     spawn_orientation = (0,0,-np.pi/2)\n",
    "# )\n",
    "\n",
    "fly0 = MovingFly(\n",
    "\n",
    "    name=\"0\", #static\n",
    "    init_pose=\"stretch\",\n",
    "    actuated_joints=all_leg_dofs,\n",
    "    control=\"position\",\n",
    "    enable_adhesion=True,\n",
    "    head_stabilization_model=\"thorax\",\n",
    "    neck_kp=1000,\n",
    "    initial_position=(8,0,0), #position\n",
    "    initial_orientation = (0,0,-np.pi/2)\n",
    ")\n",
    "\n",
    "\n",
    "arena = FlatTerrain()\n",
    "\n",
    "birdeye_cam_zoom = arena.root_element.worldbody.add(\n",
    "    \"camera\",\n",
    "    name=\"birdeye_cam_zoom\",\n",
    "    mode=\"fixed\",\n",
    "    pos=(15, 0, 20),\n",
    "    euler=(0, 0, 0),\n",
    "    fovy=45,\n",
    ")\n",
    "\n",
    "birdeye_cam = arena.root_element.worldbody.add(\n",
    "    \"camera\",\n",
    "    name=\"birdeye_cam\",\n",
    "    mode=\"fixed\",\n",
    "    pos=(15, 0, 35),\n",
    "    euler=(0, 0, 0),\n",
    "    fovy=45,\n",
    ")\n",
    "\n",
    "cam = Camera(\n",
    "    fly=fly0,\n",
    "    camera_id=\"birdeye_cam\",\n",
    "    play_speed=0.5,\n",
    "    window_size=(800, 608),\n",
    ")\n",
    "\n",
    "sim = Simulation(\n",
    "    flies=[fly0, fly],\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    "    timestep=timestep,\n",
    ")\n",
    "\n",
    "simulation_cpg = Simulation_CPG(timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/miniconda3/envs/flygymv1/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), groups=2)\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=2)\n",
       "  (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=2)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): LazyLinear(in_features=0, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################## Vision Model definition ###############################\n",
    "#to see the training -> vision.ipynb\n",
    "\n",
    "from utils import crop_hex_to_rect\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define your layers here\n",
    "        self.conv1 = nn.Conv2d(2, 8, 3, groups=2)  # Convolutional layer\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, groups=2)  # Convolutional layer\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3, groups=2)  # Convolutional layer\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Max pooling layer\n",
    "        self.fc1 = nn.LazyLinear(16)  # Lazy linear layer\n",
    "        self.fc2 = nn.Linear(16, 16)  # Linear layer\n",
    "        self.fc3 = nn.Linear(16, 2)   # Linear layer for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = self.bn2(x)  # Apply pooling after the second convolution\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.pool3(x)  # Apply pooling after the third convolution\n",
    "        x = x.flatten(1)   # Flatten the output to feed into linear layers\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Create an instance of the model\n",
    "model = Model()\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('./best_model/best_model_vision.pth'))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "########################## Initialization ###############################\n",
    "\n",
    "run_time = 1\n",
    "obs, info = sim.reset(seed=0)\n",
    "\n",
    "for i in fly.model.find_all(\"geom\"):\n",
    "    sim.physics.named.model.geom_rgba[f\"1/{i.name}\"] = (0, 0, 0, 1)\n",
    "\n",
    "second_cam_frames = []\n",
    "x = None\n",
    "alpha = 1e-1\n",
    "\n",
    "\n",
    "#usefull constant\n",
    "target_num_steps=100\n",
    "roll_finished = False\n",
    "movement_roll=False\n",
    "i_roll=0\n",
    "\n",
    "########################## Filtering ###############################\n",
    "n = 50  # Number of observations for moving average\n",
    "theta_pred_history = deque(maxlen=n)\n",
    "distances_history = deque(maxlen=n)\n",
    "\n",
    "\n",
    "########################## Standing behaviour ###############################\n",
    "standing_action = []\n",
    "for leg in legs:\n",
    "    if leg.endswith(\"M\"):\n",
    "        standing_action.extend(\n",
    "            preprogrammed_steps.get_joint_angles(leg, swing_periods[leg][1])\n",
    "        )\n",
    "    else:\n",
    "        standing_action.extend(preprogrammed_steps.get_joint_angles(leg, 0.0))\n",
    "\n",
    "fly0_action = {\"joints\": standing_action, \"adhesion\": np.array([1,0,1,1,0,1])}\n",
    "fly0.standing= standing_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:55<00:00, 181.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in trange(int(run_time / sim.timestep)):\n",
    "    fly1_action = simulation_cpg.update(fly.actuated_joints)\n",
    "    obs, _,_ ,_ , info = sim.step({\n",
    "        \"0\" : fly0_action,\n",
    "        \"1\" : fly1_action,\n",
    "    })\n",
    "\n",
    "    obs0, info0 = obs[\"0\"], info[\"0\"]\n",
    "    fly0.obs = obs0\n",
    "    render_res = sim.render()[0]\n",
    "\n",
    "    if render_res is not None:\n",
    "        fly0.visual_inputs_hist.append(obs0[\"vision\"].copy())\n",
    "        second_cam = sim.physics.bind(birdeye_cam_zoom)\n",
    "\n",
    "        x_new = sim._get_center_of_mass()[0]\n",
    "\n",
    "        if x is None:\n",
    "            x = x_new\n",
    "\n",
    "        x = (1 - alpha) * x + alpha * x_new\n",
    "\n",
    "        second_cam.pos[0] = x\n",
    "        second_img = sim.physics.render(\n",
    "            width=700, height=560, camera_id=\"birdeye_cam_zoom\"\n",
    "        )\n",
    "        second_img = cv2.putText(\n",
    "            np.ascontiguousarray(second_img),\n",
    "            f\"{sim.cameras[0].play_speed}x\",\n",
    "            org=(20, 30),\n",
    "            fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "            fontScale=0.8,\n",
    "            color=(0, 0, 0),\n",
    "            lineType=cv2.LINE_AA,\n",
    "            thickness=1,\n",
    "        )\n",
    "        second_cam_frames.append(second_img)\n",
    "        \n",
    "\n",
    "    ########################## NN Vision based on week 4 ###############################\n",
    "        \n",
    "    imgs = crop_hex_to_rect(obs0[\"vision\"]) #transform the image into a rectangle by croping\n",
    "    imgs = np.expand_dims(imgs, axis=0) #transform into a 4d vector to match the training data\n",
    "\n",
    "    with torch.no_grad(): #gradient doesn't flow back in the model\n",
    "        coords_lr_pred = model(torch.tensor(imgs)).numpy()\n",
    "\n",
    "    #prediction of the other fly's position\n",
    "    theta_pred = np.angle(coords_lr_pred @ (1, -1j) * np.exp(1j * np.pi / 4))\n",
    "    distances = np.linalg.norm(coords_lr_pred, axis=1)\n",
    "\n",
    "\n",
    "    ########################## Filtering ####################################\n",
    "    #Moving average\n",
    "    theta_pred_history.append(theta_pred)\n",
    "    distances_history.append(distances)\n",
    "\n",
    "    theta_pred_sma = np.mean(theta_pred_history)\n",
    "    distances_sma = np.mean(distances_history)\n",
    "\n",
    "\n",
    "    ########################## Workspace transcription ###############################\n",
    "\n",
    "    #a verifier mais pas sur du tout\n",
    "    if theta_pred_sma < 0:\n",
    "        side='R'\n",
    "        real_angle = theta_pred_sma + np.deg2rad(90) #offset on the leg actuator\n",
    "    else:\n",
    "        side='L'\n",
    "        real_angle = theta_pred_sma - np.deg2rad(90) #offset on the leg actuator\n",
    "\n",
    "\n",
    "    ########################## Movement computation ###############################\n",
    "        \n",
    "    if distances_sma<=8:\n",
    "        if movement_roll== False: #find roll to give to the fly\n",
    "            if side=='L':\n",
    "                roll_lin= np.linspace(obs0[\"joints\"][0][9], 1.5, target_num_steps)\n",
    "            else : \n",
    "                roll_lin= np.linspace(obs0[\"joints\"][0][29], 1.5, target_num_steps)\n",
    "            movement_roll=True\n",
    "        \n",
    "        roll = roll_lin[i_roll] if not roll_finished else 1.5 #once finished, stay at 1.5\n",
    "\n",
    "        if i_roll==0: #if iterations is 0 modulo target_num_steps -> compute the new order to smooth out the movement\n",
    "            if side=='L':\n",
    "                yaw_lin = np.linspace(obs0[\"joints\"][0][8], real_angle, target_num_steps)\n",
    "            else : \n",
    "                yaw_lin = np.linspace(obs0[\"joints\"][0][28], real_angle, target_num_steps)\n",
    "\n",
    "        fly0_action = fly0.simulate_step(sim, roll_angle=roll, yaw_angle=0, side=side) #action to follow the intruder yaw_lin[i_roll]\n",
    "\n",
    "        #update i roll and check if the movement is finished\n",
    "        i_roll+=1\n",
    "        if i_roll % target_num_steps == 0 : \n",
    "            roll_finished=True\n",
    "            i_roll=0\n",
    "\n",
    "\n",
    "    #compute the case where the fly is not in the range anymore -> linespace roll a l'envers et linespace vers 0 pour yaw\n",
    "\n",
    "    ########################## If no detection ###############################\n",
    "    else:\n",
    "        fly0_action = {\"joints\": standing_action, \"adhesion\": np.array([1,0,1,1,0,1])} #the fly stay still\n",
    "\n",
    "    \n",
    "cam.save_video(\"./outputs/cpg_controller_with_adhesion.mp4\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/cpg_controller_with_adhesion.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Video(\"./outputs/cpg_controller_with_adhesion.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdeye_cam_frames = cam._frames\n",
    "cam._frames = second_cam_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (700, 715) to (704, 720) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "sim.fly = fly0\n",
    "\n",
    "save_video_with_vision_insets(\n",
    "    sim,\n",
    "    cam,\n",
    "    \"outputs/fly_following_with_retina_images.mp4\",\n",
    "    fly0.visual_inputs_hist,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./outputs/fly_following_with_retina_images.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Video(\"./outputs/fly_following_with_retina_images.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flygym-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
